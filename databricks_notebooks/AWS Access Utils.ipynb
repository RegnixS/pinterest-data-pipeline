{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb732f55-0019-41aa-9cb9-d25defe56196",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get AWS Authentication Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8cf92ae-eeab-40d6-9fd4-d0666af622d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "295a0d8a-1786-4a43-b11a-a72dbb77ef32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f68b028-f894-4df7-a8b4-dc909945e7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a5f614b-de83-4f3e-99ff-68a27a9225c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-129a67850695-bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed034ddb-6656-434f-9e30-4c70933ad526",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define function read_from_S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6eeb7fbc-cf6a-4392-bdc5-70ad4a700983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_from_s3(file_type, mount=False):\n",
    "    '''\n",
    "    This function reads json data from an S3 bucket and returns a dataframe.\n",
    "\n",
    "    Args:\n",
    "        file_type (string) : The type of json data to be read (pin, geo or user).\n",
    "        mount (Boolean) : Whether the bucket is mounted or not.\n",
    "    '''\n",
    "    # File location and type\n",
    "    # Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "    file_location = f\"s3n://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{AWS_S3_BUCKET}/topics/129a67850695.{file_type}/partition=0/*.json\"\n",
    "    if mount:\n",
    "        file_location = f\"/mnt/user-129a67850695-bucket/topics/129a67850695.{file_type}/partition=0/*.json\"\n",
    "    # Read from\n",
    "    file_type = \"json\"\n",
    "    # Ask Spark to infer the schema\n",
    "    infer_schema = \"true\"\n",
    "    # Read in JSONs from mounted S3 bucket\n",
    "    df_out = spark.read.format(file_type) \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .load(file_location)\n",
    "\n",
    "    # Display Spark dataframe to check its content\n",
    "    display(df_out)\n",
    "\n",
    "    df_out.createOrReplaceGlobalTempView(f\"gtv_129a67850695_{file_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c39ccc-e65c-49af-9bd8-7bb39cf21fab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define function read_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44dfb45e-3fc9-4603-a9b5-b05cd57826ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_stream(stream_name, schema):\n",
    "    '''\n",
    "    This function reads data from a Kinesis stream, extracts the columns from the \"data\" column and returns a datframe with the schema provided.\n",
    "\n",
    "    Args:\n",
    "        stream_name (string) : The name of the stream to read from.\n",
    "        schema (string) : A string containing the schema of the output dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame : A DataFrame with the provided schema.\n",
    "    '''\n",
    "    from pyspark.sql.functions import col, from_json\n",
    "    \n",
    "    # Read in the Kinesis stream to a dataframe\n",
    "    df_kinesis = spark.readStream \\\n",
    "        .format('kinesis') \\\n",
    "        .option('streamName', stream_name) \\\n",
    "        .option('initialPosition','earliest') \\\n",
    "        .option('region','us-east-1') \\\n",
    "        .option('awsAccessKey', ACCESS_KEY) \\\n",
    "        .option('awsSecretKey', SECRET_KEY) \\\n",
    "        .load()\n",
    "\n",
    "    # Create a new dataframe containing the columns exploded from the \"data\" column\n",
    "    df_out = df_kinesis.select(from_json(col(\"data\").cast(\"string\"), schema).alias(\"data\")) \\\n",
    "        .select(\"data.*\")\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01f4a025-e5de-4a28-b453-fd9fd883abc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define function write_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a329b3-7461-4df2-8167-3b7b7db1dec7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_stream(table_name, df_in):\n",
    "    '''\n",
    "    This function writes data from a kinesis stream to a delta table.\n",
    "\n",
    "    Args:\n",
    "        table_name (string) : The name of the delta table to write to.\n",
    "        df_in (pyspark.sql.DataFrame) : The dataframe to be written to the table.\n",
    "    '''\n",
    "    # Write the cleaned dataframe to a Delta Table \n",
    "    df_in.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", f\"/tmp/kinesis/{table_name}_checkpoints/\") \\\n",
    "        .table(table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AWS Access Utils",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
