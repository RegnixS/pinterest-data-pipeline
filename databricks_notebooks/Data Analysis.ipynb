{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11774aa3-7012-4f95-8f70-0eefe069c7d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Analysis\n",
    "1. Create Dataframes from Global Temporary Views\n",
    "2. Most popular category in each country\n",
    "3. Most popular category each year\n",
    "4. Most followers in each country\n",
    "5. Most popular category for different age groups\n",
    "6. Median follower count for different age groups\n",
    "7. How many users have joined each year\n",
    "8. Median follow count of users based on their joining year\n",
    "9. Median follow count of users based on their joining year and age group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cd51f5-cdc1-42f3-809a-a8bae0485127",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create Dataframes from Global Temporary Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61369e92-c768-460f-ae40-51ba3d8e11b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pin = spark.table(\"global_temp.gtv_129a67850695_pin_clean\")\n",
    "df_geo = spark.table(\"global_temp.gtv_129a67850695_geo_clean\")\n",
    "df_user = spark.table(\"global_temp.gtv_129a67850695_user_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c7c946a-f0ec-40e8-adb4-86f06d5c6667",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Function to find the most popular category for a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57da268-bee4-43ac-9556-bfa21d4e7e67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import desc, rank\n",
    "def most_popular_category(df_in, partition):\n",
    "    '''\n",
    "    This function will return a data frame showing only the most popular categories for any given column\n",
    "\n",
    "    Args:\n",
    "        df_in (pyspark.sql.DataFrame) : A DataFrame containing a column to partition by and a category_count column\n",
    "        partition (string) : The column in which to partition by category_count\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame : A DataFrame of the data.\n",
    "    '''\n",
    "    # Create a window that orders by descending \"category_count\" within each partition\n",
    "    window_spec = Window.partitionBy(partition).orderBy(desc(\"category_count\"))\n",
    "    # Rank \"category_count\" using the window\n",
    "    # Only show the highest rank\n",
    "    # Select the columns for the output\n",
    "    df_out = df_in.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "        .filter(\"rank = 1\") \\\n",
    "        .select(partition, \"category\", \"category_count\") \\\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ff958b-51a7-41e8-a1a0-6898c1332d60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Most popular category in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef9adaf-5ee9-48a6-9b27-d57b846bb1c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join df_pin and df_geo together using common column \"ind\"\n",
    "# Count number of pins for each \"country\" + \"category\" combination\n",
    "# Rename the count column\n",
    "df_pop_category_by_country = df_pin.join(df_geo, df_pin[\"ind\"] == df_geo[\"ind\"]) \\\n",
    "    .groupBy(\"country\", \"category\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"category_count\") \n",
    "\n",
    "# Call the most_popular_category function passing in \"country\"\n",
    "df_pop_category_by_country = most_popular_category(df_pop_category_by_country, \"country\") \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c4249e2-efd7-40dc-892d-71a81c0a60e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Most popular category each year (between 2018 and 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f638a33-24e6-4ea2-9258-2bed9b5230f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "# Join df_pin and df_geo together using common column \"ind\"\n",
    "# Use year() function to extract just the year part of the timestamp to \"post_year\"\n",
    "# Count number of pins for each \"post_year\" + \"category\" combination\n",
    "# Rename the count column\n",
    "# Filter between 2018 and 2022\n",
    "df_pop_category_by_year = df_pin.join(df_geo, df_pin[\"ind\"] == df_geo[\"ind\"]) \\\n",
    "    .withColumn(\"post_year\", year(\"timestamp\")) \\\n",
    "    .groupBy(\"post_year\", \"category\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"category_count\") \\\n",
    "    .filter(\"post_year >= 2018 and post_year <= 2022\")\n",
    "\n",
    "# Call the most_popular_category function passing in \"post_year\"\n",
    "df_pop_category_by_year = most_popular_category(df_pop_category_by_year, \"post_year\") \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ba27af-b4c0-4ec9-9e1c-3ce533c53fe0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Users with the most followers in each country\n",
    "Step 1: For each country find the user with the most followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "395b7a6a-10c6-4415-b53e-0fb431a0073d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import desc, rank\n",
    "# Create a window that orders by descending \"follower_count\" for each \"country\"\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(desc(\"follower_count\"))\n",
    "# Join df_pin and df_geo together using common column \"ind\"\n",
    "# Rank \"follower_count\" using the window\n",
    "# Only show the highest rank\n",
    "# Select the columns for the output\n",
    "# Drop duplicates as there can be multiple pins by the same poster\n",
    "df_most_followers_by_country = df_pin.join(df_geo, df_pin[\"ind\"] == df_geo[\"ind\"]) \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(\"rank = 1\") \\\n",
    "    .select(\"country\", \"poster_name\", \"follower_count\") \\\n",
    "    .dropDuplicates([\"poster_name\", \"country\"])\n",
    "display(df_most_followers_by_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2be4060-9dc2-494f-a2e0-6a006610425f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The user with the most followers in all countries\n",
    "Step 2: Based on the above query, find the country with the user with most followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3771ab-9988-45cf-840a-3476437d6ac9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import desc, rank\n",
    "# Create a window that orders by descending \"follower_count\" over the whole dataframe\n",
    "window_spec = Window.partitionBy().orderBy(desc(\"follower_count\"))\n",
    "# Rank \"follower_count\" using the window\n",
    "# Only show the highest rank\n",
    "# Select the columns for the output\n",
    "df_most_followers_overall = df_most_followers_by_country.withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .filter(\"rank = 1\") \\\n",
    "    .select(\"country\", \"poster_name\", \"follower_count\") \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362485e8-a74a-4f45-938e-689d97388e49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Most popular category for different age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2384a944-ee7f-40bc-927b-d1834c827cb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "# Use the conditional when clause to bin ages into age groups\n",
    "df_user_with_age_groups = df_user.withColumn(\"age_group\", when(df_user[\"age\"].between(18,24), \"18-24\") \\\n",
    "   .when(df_user[\"age\"].between(25,35), \"25-35\") \\\n",
    "   .when(df_user[\"age\"].between(36,50), \"36-50\") \\\n",
    "   .otherwise(\"50+\")\n",
    ")\n",
    "# Join df_pin and df_user_with_age_groups together using common column \"ind\"\n",
    "# Count number of pins for each \"age_group\" + \"category\" combination\n",
    "# Rename the count column\n",
    "df_pop_category_by_age_groups = df_pin.join(df_user_with_age_groups, df_pin[\"ind\"] == df_user_with_age_groups[\"ind\"]) \\\n",
    "    .groupBy(\"age_group\", \"category\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"category_count\")\n",
    "\n",
    "# Call the most_popular_category function passing in \"age_group\"\n",
    "df_pop_category_by_age_groups = most_popular_category(df_pop_category_by_age_groups, \"age_group\") \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9023d2d-5fe6-42cf-8253-dc610b16c7b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Median follower count for different age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29e12a1a-a0f9-4264-9276-1716a67b9c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percentile_approx\n",
    "# df_user_with_age_groups contains duplicate users because it's based on pin data. Drop the duplicates first so median is calculated correctly\n",
    "# Join df_pin and df_user_with_age_groups together using common column \"ind\"\n",
    "# Find median values of \"follower_count\" for each \"age_group\"\n",
    "# median() function has been deprecated, so we use percentile_approx() instead where 0.5 is the halfway point like median\n",
    "# Order by \"age_group\"\n",
    "df_median_follower_counts_by_age_group = df_pin.join(\n",
    "        df_user_with_age_groups.dropDuplicates([\"user_name\",\"age\"]), \n",
    "        df_pin[\"ind\"] == df_user_with_age_groups[\"ind\"]\n",
    "    ) \\\n",
    "    .groupby(\"age_group\").agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .orderBy(\"age_group\") \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1e9a4ee-09a2-4352-8504-40a43cffb00f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## How many users join each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "143cee59-4552-46e9-90aa-3d07c82810d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "# Drop duplicate users from df_user\n",
    "# Use year() function to extract just the year part of the timestamp to \"post_year\"\n",
    "# Count number of distinct users for each \"post_year\"\n",
    "# Rename the count column\n",
    "# Filter between 2015 and 2020\n",
    "df_date_joined_by_year = df_user.dropDuplicates([\"user_name\",\"age\"]) \\\n",
    "    .withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    "    .groupBy(\"post_year\") \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"number_users_joined\") \\\n",
    "    .filter(\"post_year >= 2015 and post_year <= 2020\") \\\n",
    "    .display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6415c5-f41f-4d3e-8457-ae5df7e382a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Median follower count of users based on joining year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7edcdb-a37a-4249-bd46-19e38c965e22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percentile_approx\n",
    "# df_user contains duplicate users because it's based on pin data. Drop the duplicates first so median is calculated correctly\n",
    "# Join df_pin and df_user together using common column \"ind\"\n",
    "# Find median values of \"follower_count\" for each \"post_year\"\n",
    "# median() function has been deprecated, so we use percentile_approx() instead where 0.5 is the halfway point like median\n",
    "# Order by \"post_year\"\n",
    "# Filter between 2015 and 2020\n",
    "df_median_follower_counts_by_joining_year = df_pin.join(\n",
    "        df_user.dropDuplicates([\"user_name\",\"age\"]),\n",
    "        df_pin[\"ind\"] == df_user[\"ind\"]\n",
    "    ) \\\n",
    "    .withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    "    .groupby(\"post_year\").agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .orderBy(\"post_year\") \\\n",
    "    .filter(\"post_year >= 2015 and post_year <= 2020\") \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d25c93-69bc-4c7d-ad8d-b0ddab2e783c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Median follower count of users based on joining year and age group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c01189-129b-4497-9004-1cd409000919",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import percentile_approx\n",
    "# df_user_with_age_groups contains duplicate users because it's based on pin data. Drop the duplicates first so median is calculated correctly\n",
    "# Join df_pin and df_user_with_age_groups together using common column \"ind\"\n",
    "# Find median values of \"follower_count\" for each \"age_group\" and \"post_year\" combination\n",
    "# median() function has been deprecated, so we use percentile_approx() instead where 0.5 is the halfway point like median\n",
    "# Order by \"age_group\" and \"post_year\"\n",
    "# Filter between 2015 and 2020\n",
    "df_median_follower_counts_by_joining_year_and_age_group = df_pin.join(\n",
    "        df_user_with_age_groups.dropDuplicates([\"user_name\",\"age\"]),\n",
    "        df_pin[\"ind\"] == df_user_with_age_groups[\"ind\"]\n",
    "    ) \\\n",
    "    .withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    "    .groupby(\"age_group\", \"post_year\").agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\")) \\\n",
    "    .orderBy(\"age_group\", \"post_year\") \\\n",
    "    .filter(\"post_year >= 2015 and post_year <= 2020\") \\\n",
    "    .display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
